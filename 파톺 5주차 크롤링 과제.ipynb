{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 과제. BeautifulSoup을 이용하여 아래의 세 기사의 제목과 언론사를 크롤링해주세요!\n",
    "* for문을 이용하여 url 각각의 기사제목과 언론사를 가져와주세요\n",
    "* 가져온 정보를 아래와 같이 DataFrame으로 만들어 주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|title|company|url|\n",
    "|------|---|---|\n",
    "|13일부터 마스크 착용 의무화..한 달 계도 후 과태료 10만 원|YTN|https://news.v.daum.net/v/20201004215700006|\n",
    "|\"사망 10대와 같은 곳서 같은 백신 접종한 32명, 이상반응 없어\"|연합뉴스|https://news.v.daum.net/v/20201020153505519|\n",
    "|지하수에 사는 '골룸 가물치'야, 넌 어디서 왔니|한겨레|https://news.v.daum.net/v/20201020153609574|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_url = [\"https://news.v.daum.net/v/20201004215700006\", \"https://news.v.daum.net/v/20201020153505519\", \"https://news.v.daum.net/v/20201020153609574\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs 가져오기\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우선 for문을 어떻게 해야할지 몰라, 크롤링으로 각 기사 정보를 가져와봄\n",
    "res1 = requests.get(\"https://news.v.daum.net/v/20201004215700006\")\n",
    "soup1 = bs(res1.content, 'html.parser')\n",
    "list1 = [soup1.select('#cSub > div > h3')[0].text, \n",
    "         (soup1.select('#mArticle > div.foot_view > div.cp_view > strong')[0].text).split()[0]]\n",
    "\n",
    "res2 = requests.get(\"https://news.v.daum.net/v/20201020153505519\")\n",
    "soup2 = bs(res2.content, 'html.parser')\n",
    "list2 = [soup2.select('#cSub > div > h3')[0].text, \n",
    "         (soup2.select('#mArticle > div.foot_view > div.cp_view > strong')[0].text).split()[0]]\n",
    "\n",
    "res3 = requests.get(\"https://news.v.daum.net/v/20201020153609574\")\n",
    "soup3 = bs(res3.content, 'html.parser')\n",
    "list3 = [soup3.select('#cSub > div > h3')[0].text, \n",
    "         (soup3.select('#mArticle > div.foot_view > div.cp_view > strong')[0].text).split()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13일부터 마스크 착용 의무화..한 달 계도 후 과태료 10만 원</td>\n",
       "      <td>YTN</td>\n",
       "      <td>https://news.v.daum.net/v/20201004215700006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"사망 10대와 같은 곳서 같은 백신 접종한 32명, 이상반응 없어\"</td>\n",
       "      <td>연합뉴스</td>\n",
       "      <td>https://news.v.daum.net/v/20201020153505519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>지하수에 사는 '골룸 가물치'야, 넌 어디서 왔니</td>\n",
       "      <td>한겨레</td>\n",
       "      <td>https://news.v.daum.net/v/20201020153609574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    title company  \\\n",
       "0    13일부터 마스크 착용 의무화..한 달 계도 후 과태료 10만 원     YTN   \n",
       "1  \"사망 10대와 같은 곳서 같은 백신 접종한 32명, 이상반응 없어\"    연합뉴스   \n",
       "2             지하수에 사는 '골룸 가물치'야, 넌 어디서 왔니     한겨레   \n",
       "\n",
       "                                           url  \n",
       "0  https://news.v.daum.net/v/20201004215700006  \n",
       "1  https://news.v.daum.net/v/20201020153505519  \n",
       "2  https://news.v.daum.net/v/20201020153609574  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 결과를 수작업으로 넣어봄\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'title':[soup1.select('#cSub > div > h3')[0].text, \n",
    "                            soup2.select('#cSub > div > h3')[0].text, \n",
    "                            soup3.select('#cSub > div > h3')[0].text],\n",
    "                   'company':[(soup1.select('#mArticle > div.foot_view > div.cp_view > strong')[0].text).split()[0],\n",
    "                              (soup2.select('#mArticle > div.foot_view > div.cp_view > strong')[0].text).split()[0],\n",
    "                              (soup3.select('#mArticle > div.foot_view > div.cp_view > strong')[0].text).split()[0]],\n",
    "                   'url':crawl_url})\n",
    "# 이렇게 나오긴 함\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['title', 'company', 'url'])\n",
    "\n",
    "for i in crawl_url:\n",
    "    # 혹시 res랑 soup를 for문 안에 넣어도 되나 했는데, 됐다.\n",
    "    res = requests.get(i)\n",
    "    soup = bs(res.content, 'html.parser')\n",
    "    # 그렇다면 각 요소도 변수로 지정을 해본다.\n",
    "    title = soup.select('#cSub > div > h3')[0].text\n",
    "    company = (soup.select('#mArticle > div.foot_view > div.cp_view > strong')[0].text).split()[0]\n",
    "    url = i\n",
    "    # 구글의 힘을 빌려 데이터 프레임 안에 넣어본다.\n",
    "    df = df.append(pd.DataFrame([[title, company, url]], columns=['title', 'company', 'url']), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 내용은 조교님의 힌트를 빌렸다.\n",
    "df.set_index('title', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13일부터 마스크 착용 의무화..한 달 계도 후 과태료 10만 원</th>\n",
       "      <td>YTN</td>\n",
       "      <td>https://news.v.daum.net/v/20201004215700006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"사망 10대와 같은 곳서 같은 백신 접종한 32명, 이상반응 없어\"</th>\n",
       "      <td>연합뉴스</td>\n",
       "      <td>https://news.v.daum.net/v/20201020153505519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>지하수에 사는 '골룸 가물치'야, 넌 어디서 왔니</th>\n",
       "      <td>한겨레</td>\n",
       "      <td>https://news.v.daum.net/v/20201020153609574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       company  \\\n",
       "title                                            \n",
       "13일부터 마스크 착용 의무화..한 달 계도 후 과태료 10만 원       YTN   \n",
       "\"사망 10대와 같은 곳서 같은 백신 접종한 32명, 이상반응 없어\"    연합뉴스   \n",
       "지하수에 사는 '골룸 가물치'야, 넌 어디서 왔니                한겨레   \n",
       "\n",
       "                                                                                url  \n",
       "title                                                                                \n",
       "13일부터 마스크 착용 의무화..한 달 계도 후 과태료 10만 원    https://news.v.daum.net/v/20201004215700006  \n",
       "\"사망 10대와 같은 곳서 같은 백신 접종한 32명, 이상반응 없어\"  https://news.v.daum.net/v/20201020153505519  \n",
       "지하수에 사는 '골룸 가물치'야, 넌 어디서 왔니             https://news.v.daum.net/v/20201020153609574  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 일단 했지만 더 해보고 싶은 것\n",
    "- 클릭 가능하도록 링크를 살린다.\n",
    "- title이 인덱스가 되어서 밑으로 내려가서 이상한데, 문제 예시처럼 깔끔하게 만들고 싶다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['title', 'company', 'url'])\n",
    "\n",
    "for i in crawl_url:\n",
    "    # 혹시 res랑 soup를 for문 안에 넣어도 되나 했는데, 됐다.\n",
    "    res = requests.get(i)\n",
    "    soup = bs(res.content, 'html.parser')\n",
    "    # 그렇다면 각 요소도 변수로 지정을 해본다.\n",
    "    title = soup.select('#cSub > div > h3')[0].text\n",
    "    company = (soup.select('#mArticle > div.foot_view > div.cp_view > strong')[0].text).split()[0]\n",
    "    url = i\n",
    "    # 구글의 힘을 빌려 데이터 프레임 안에 넣어본다.\n",
    "    df = df.append(pd.DataFrame([[title, company, url]], columns=['title', 'company', 'url']), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list 다시 연습해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [12,2,4,6,6,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 2, 4, 6, 6, 3, 2]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.append('2d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 2, 4, 6, 6, 3, 2, '2d']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
